---
title: Introduction
description: "OM1 is A modular AI runtime for agents and robots with capabilities including movement and speech."
---

<img
  className="block dark:hidden"
  src="/assets/openmind-intro-cover.png"
  alt="Hero Light"
/>
<img
  className="hidden dark:block"
  src="/assets/openmind-intro-cover.png"
  alt="Hero Dark"
/>


## What is OpenMind (OM1)
OM1 allows AI agents to be configured and then deployed in both the digital and physical world. You can create *one* AI agent and run it in the cloud but also on physical robot hardware such as Quadrupeds, and, soon, TurtleBot 3 and Humanoids. 

For example, an AI agent built on OM1 can ingest data from multiple sources (the web, X/Twitter, cameras, and LIDAR) and can then Tweet and explore your house, shake your hand, or talk to you. In another example, with OM1, you can talk with OpenAI's `gpt-4o` and literally shake hands with it.

## Capabilities of OM1

<CardGroup cols={2}>
  <Card
    title=" Simple, modular architecture"
    icon="square-1"
  >
  Easy-to-understand, independent components that work together seamlessly.
   
  </Card>
   <Card
    title="All python "
    icon="square-2"
  >
  Independent modules that are easy maintain, and extend.
  </Card>
   <Card
    title="Easy to add new data inputs "
    icon="square-3"
  >
    Seamlessly integrate new data without major changes to the existing structure.
  </Card>
  <Card
    title="Easy to support new hardware "
    icon="square-4"
  >
   via plugins for API endpoints and specific robot hardware
  </Card>
  <Card
    title="Can be connected to: "
    icon="square-5"
  >
   `ROS2`, `Zenoh`, and `CycloneDDS`
  </Card>
  <Card
    title="Includes a simple web-based debug display "
    icon="square-6"
  >
   to watch the system work (`WebSim` at http://localhost:8000)
  </Card>
  <Card
    title="Preconfigured endpoints"
    icon="square-7"
  >
  for Voice-to-Speech, OpenAI's `gpt-4o`, DeepSeek, and multiple VLMs
  </Card>
  </CardGroup>